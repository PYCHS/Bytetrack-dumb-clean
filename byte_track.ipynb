{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmMrf7WuTRVa",
    "outputId": "00c99a3c-37af-4857-b3b5-562e5567090f"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYY2AyDFTV3J",
    "outputId": "a910f16b-be1d-4b24-d1d0-fb8f2f3b6f44"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "KITTI_PATH = f\"{HOME}/refer-kitti/KITTI/training/image_02/\"\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNX8tr4FWOFd"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMTAen_0WQtm",
    "outputId": "96a5e403-5956-43b4-b505-90fdb9a97175"
   },
   "outputs": [],
   "source": [
    "!gdown -O \"0005.zip\" \"https://drive.google.com/uc?id=1aJx81SILqLSjLzyLUUcLntDxWl2JDSSW\"\n",
    "!unzip \"/content/0005.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwDpolPCTaLB"
   },
   "source": [
    "# Use Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-feAIgKRTYyb",
    "outputId": "2317931e-53a9-4c62-b018-56d7e047ca46"
   },
   "outputs": [],
   "source": [
    "! pip install -q ultralytics utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOQpG0QpTcKS",
    "outputId": "7230968e-db0b-4eaa-fb53-0f6c7c0e6561"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "m8e-q_RiTt39",
    "outputId": "b271e2c4-4e77-4266-9480-5deeedc00b97"
   },
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "# model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "# results = model(\"/content/0006/000000.png\")  # predict on an image\n",
    "results = model(f\"{KITTI_PATH}/0011/000211.png\")  # predict on an image on local disk\n",
    "\n",
    "# Access the results\n",
    "for result in results:\n",
    "    xywh = result.boxes.xywh  # center-x, center-y, width, height\n",
    "    xywhn = result.boxes.xywhn  # normalized\n",
    "    xyxy = result.boxes.xyxy  # top-left-x, top-left-y, bottom-right-x, bottom-right-y\n",
    "    xyxyn = result.boxes.xyxyn  # normalized\n",
    "    names = [result.names[cls.item()] for cls in result.boxes.cls.int()]  # class name of each box\n",
    "    confs = result.boxes.conf  # confidence score of each box\n",
    "    print(int(result.boxes[0].cls[0]))\n",
    "\n",
    "    img = result.plot()\n",
    "    plt.imshow(img[..., ::-1])\n",
    "    plt.axis('off')\n",
    "    print(xyxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcooYaKtZjfT"
   },
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sirmfCDdZ81R",
    "outputId": "ec1f4493-6a0d-4932-8297-f5d9cc8d0456"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n",
    "results = model.track(source=f\"{KITTI_PATH}/0005\", tracker=\"bytetrack.yaml\",persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XkhJ4RtRlEvn",
    "outputId": "bfde1e5c-1569-46b5-b43b-70de4b7208ac"
   },
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Access the Results object for the first frame\n",
    "    first_frame_results = results[0]\n",
    "\n",
    "    # Check if there are any detected boxes in the first frame\n",
    "    if first_frame_results.boxes is not None:\n",
    "        print(\"Detections and Tracks for the First Frame:\")\n",
    "        for box in first_frame_results.boxes:\n",
    "            # Bounding box coordinates (xyxy format)\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            print(\"id: \",box.id[0])\n",
    "\n",
    "            # Class ID and Name\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            class_name = first_frame_results.names[class_id]\n",
    "\n",
    "            # Confidence Score\n",
    "            confidence = float(box.conf[0].cpu().numpy())\n",
    "\n",
    "            # Tracker ID (important for tracking results)\n",
    "            tracker_id = int(box.id[0].cpu().numpy()) if box.id is not None else \"N/A\"\n",
    "\n",
    "            print(f\"  Object ID: {tracker_id}, Class: {class_name} (ID: {class_id}), Confidence: {confidence:.2f}, BBox: [{x1}, {y1}, {x2}, {y2}]\")\n",
    "    else:\n",
    "        print(\"No detections in the first frame.\")\n",
    "else:\n",
    "    print(\"No results found. Ensure model.track() was executed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Class\n",
    "- `write_gt()`\n",
    "- `save_crops_from_detections()`\n",
    "- `inference()`\n",
    "- `Crop_Img`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2\n",
    "import shutil\n",
    "from LLM import select_targets\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Crop_Img:\n",
    "    frame_id: int\n",
    "    tracker_id: int\n",
    "    bbox: dict  # {'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n",
    "    width: int\n",
    "    height: int\n",
    "    crop_path: str\n",
    "    cls: int\n",
    "    \n",
    "    @property\n",
    "    def x1(self):\n",
    "        return self.bbox.get('x1', 0)\n",
    "    \n",
    "    @property\n",
    "    def y1(self):\n",
    "        return self.bbox.get('y1', 0)\n",
    "    \n",
    "    @property\n",
    "    def x2(self):\n",
    "        return self.bbox.get('x2', 0)\n",
    "    \n",
    "    @property\n",
    "    def y2(self):\n",
    "        return self.bbox.get('y2', 0)\n",
    "    # @property\n",
    "    # def cls(self):\n",
    "    #     return self.bbox.get('cls', 0)\n",
    "    def get_class(self, cls):\n",
    "            if cls==0:\n",
    "                return \"person\"\n",
    "            elif cls==1:\n",
    "                return \"bicycle\"\n",
    "            elif cls==2:\n",
    "                return \"car\"\n",
    "            elif cls==3:      \n",
    "                return \"motorcycle\"\n",
    "            elif cls==5:\n",
    "                return \"bus\"\n",
    "            elif cls==7:\n",
    "                return \"truck\"\n",
    "            else:\n",
    "                return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_bbox(x1, y1, x2, y2, img_w, img_h, min_w=50, min_h=100):\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    cx = x1 + w / 2\n",
    "    cy = y1 + h / 2 \n",
    "\n",
    "    new_w = max(w, min_w)\n",
    "    new_h = max(h, min_h)\n",
    "\n",
    "    new_x1 = cx - new_w / 2\n",
    "    new_x2 = cx + new_w / 2\n",
    "    new_y1 = cy - new_h / 2\n",
    "    new_y2 = cy + new_h / 2 \n",
    "\n",
    "    new_x1 = max(0, new_x1)\n",
    "    new_y1 = max(0, new_y1)\n",
    "    new_x2 = min(img_w, new_x2)\n",
    "    new_y2 = min(img_h, new_y2)\n",
    "\n",
    "    # Horizontal adjustments\n",
    "    if new_x1 < 0: \n",
    "        # Too far left? Snap start to 0, end to min_w\n",
    "        new_x1 = 0\n",
    "        new_x2 = new_w\n",
    "    elif new_x2 > img_w:\n",
    "        # Too far right? Snap end to img_w, start to img_w - new_w\n",
    "        new_x2 = img_w\n",
    "        new_x1 = img_w - new_w\n",
    "\n",
    "    # Vertical adjustments\n",
    "    if new_y1 < 0:\n",
    "        new_y1 = 0\n",
    "        new_y2 = new_h\n",
    "    elif new_y2 > img_h:\n",
    "        new_y2 = img_h\n",
    "        new_y1 = img_h - new_h\n",
    "    return new_x1, new_y1, new_x2, new_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_crops_from_detections(box, frame_id, crops_dir, video_id):\n",
    "  \"\"\"Save cropped images of detected objects for LLM analysis.\"\"\"\n",
    "  ORI_PATH = f\"{HOME}/refer-kitti/KITTI/training/image_02/{video_id}\"\n",
    "  img_path = os.path.join(ORI_PATH, f\"{frame_id-1:06d}.png\")\n",
    "  img = cv2.imread(img_path)\n",
    "  min_w, min_h = 50, 100 \n",
    "  seq_h, seq_w, _ = img.shape\n",
    "\n",
    "  os.makedirs(crops_dir, exist_ok=True)\n",
    "  crop = None\n",
    "  if box :\n",
    "    x1, y1, x2, y2 = list(map(int, box.xyxy[0]))\n",
    "    new_x1, new_y1, new_x2, new_y2 = adjust_bbox(x1, y1, x2, y2, seq_w, seq_h, min_w, min_h)\n",
    "      \n",
    "    crop = img[new_y1:new_y2, new_x1:new_x2]\n",
    "  if crop is not None and crop.size > 0 and box.id is not None:  # Make sure crop is valid\n",
    "      save_path = os.path.join(crops_dir, f\"i{int(box.id[0])}_f{frame_id}.jpg\")\n",
    "      cv2.imwrite(save_path, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gt(gt_path, save_path, expression_path, labels_path, video_id, expression):\n",
    "  save_format = \"{frame},{id},{x1},{y1},{w},{h},1, 1, 1\\n\"\n",
    "\n",
    "  with open(os.path.join(expression_path, video_id, expression+\".json\")) as f:\n",
    "      json_info = json.load(f)\n",
    "\n",
    "  # FIX: Get image dimensions from the first available frame\n",
    "  ORI_PATH = f\"{HOME}/refer-kitti/KITTI/training/image_02/{video_id}\"\n",
    "  first_frame_id = int(list(json_info[\"label\"].keys())[0])\n",
    "  img_path = os.path.join(ORI_PATH, f\"{first_frame_id:06d}.png\")\n",
    "  img = cv2.imread(img_path)\n",
    "  seq_h, seq_w, _ = img.shape\n",
    "\n",
    "  labels_path = os.path.join(labels_path, video_id)\n",
    "  if(not os.path.exists(os.path.join(save_path,video_id,expression)) ):\n",
    "    os.mkdir(os.path.join(save_path,video_id,expression))\n",
    "#   print(f\"json_info:{json_info}\")\n",
    "  with open(os.path.join(save_path, video_id, expression, \"gt.txt\"), \"w\") as f:\n",
    "      for k in json_info[\"label\"].keys():\n",
    "          frame_id = int(k)\n",
    "          if not os.path.isfile(\n",
    "              os.path.join(labels_path, \"{:06d}.txt\".format(frame_id))\n",
    "          ):\n",
    "              continue\n",
    "          frame_gt = np.loadtxt(\n",
    "              os.path.join(labels_path, \"{:06d}.txt\".format(frame_id))\n",
    "          ).reshape(-1, 6)\n",
    "          for frame_gt_line in frame_gt:\n",
    "              aa = json_info[\"label\"][k]  # all gt from frame\n",
    "              aa = [int(a) for a in aa]\n",
    "              if int(frame_gt_line[1]) in aa:  # choose referent gt from all gt\n",
    "                  track_id = int(frame_gt_line[1])\n",
    "                  x1, y1, w, h = frame_gt_line[2:6]  # KITTI -> [x1, y1, w, h]\n",
    "                  line = save_format.format(\n",
    "                      frame=frame_id + 1,\n",
    "                      id=track_id,\n",
    "                      x1=x1 * seq_w,\n",
    "                      y1=y1 * seq_h,\n",
    "                      w=w * seq_w,\n",
    "                      h=h * seq_h,\n",
    "                  )\n",
    "                  f.write(line)\n",
    "\n",
    "  print(\"save gt to {}\".format(os.path.join(save_path, video_id, expression, \"gt.txt\")))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(EXP_PATH, EXPRESSION_PATH, seq, results, video_id):\n",
    "    SEQ_PATH = os.path.join(EXPRESSION_PATH, seq[0], seq[1])\n",
    "    save_format = \"{frame},{id},{x1},{y1},{w},{h},1,1,1\\n\"\n",
    "    with open(SEQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt = json.load(f)\n",
    "        sentence = prompt.get(\"sentence\", \"\")\n",
    "\n",
    "    expression_str = seq[1].split(\".\")[0]\n",
    "    exp_dir = os.path.join(EXP_PATH, seq[0], expression_str)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    predict_path = os.path.join(exp_dir, \"predict.txt\")\n",
    "\n",
    "    os.makedirs(\"crops/\", exist_ok=True)\n",
    "\n",
    "    first_frame_path = os.path.join(KITTI_PATH, seq[0], f\"{1:06d}.png\")\n",
    "    img = cv2.imread(first_frame_path)\n",
    "    seq_h, seq_w = img.shape[:2]\n",
    "    existing_ids = set()  # To keep track of existing tracker IDs\n",
    "\n",
    "    with open(predict_path, \"w\") as fout:\n",
    "      for frame_idx, result in enumerate(tqdm(results), start=1):\n",
    "        crop_imgs = []\n",
    "        \n",
    "\n",
    "        for box in result.boxes:\n",
    "\n",
    "          # save crops for this frame\n",
    "          save_crops_from_detections(box, frame_idx, \"crops/\", video_id)\n",
    "          # print(list(map(int, box.xyxy[0])))\n",
    "          # Record bounding box position\n",
    "          x1, y1, x2, y2 = list(map(float, box.xyxy[0]))\n",
    "          tracker_id = int(box.id[0]) if box.id is not None else -1\n",
    "\n",
    "          # Create Crop_Img object using the defined class\n",
    "          crop_img_obj = Crop_Img(\n",
    "            frame_id=frame_idx,\n",
    "            tracker_id=tracker_id,\n",
    "            bbox={\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "            width=x2 - x1,\n",
    "            height=y2 - y1,\n",
    "            crop_path=os.path.join(\n",
    "              \"crops/\", f\"i{tracker_id}_f{frame_idx}.jpg\"\n",
    "            ),\n",
    "            cls=box.cls[0]\n",
    "          )\n",
    "          crop_imgs.append(crop_img_obj)\n",
    "\n",
    "        # Call select_targets on first frame, every 30 frames, or when new IDs appear\n",
    "          current_ids = {int(crop_img.tracker_id) for crop_img in crop_imgs}\n",
    "          new_ids = current_ids - existing_ids\n",
    "          \n",
    "          if (frame_idx == 1) or (frame_idx % 11 == 0): #or (new_ids and crop_imgs):\n",
    "            matches = select_targets(\n",
    "              crops_dir=\"crops/\",\n",
    "              prompt=sentence,\n",
    "              quiet=False,\n",
    "              img_size={\"img_h\": seq_h, \"img_w\": seq_w},\n",
    "              crop_info=crop_imgs  # Pass list of Crop_Img objects\n",
    "            )\n",
    "            existing_ids.update(matches)  # Update existing IDs with matches\n",
    "          else:\n",
    "            matches = existing_ids # Keep track of existing IDs\n",
    "        # Clean up crops directory\n",
    "        if os.path.exists(\"crops/\"):\n",
    "          shutil.rmtree(\"crops/\")\n",
    "        os.makedirs(\"crops/\", exist_ok=True)\n",
    "\n",
    "        # Write predictions for matched objects with actual bounding boxes\n",
    "        for crop_img in crop_imgs:\n",
    "          if crop_img.tracker_id in matches:\n",
    "            x1 = crop_img.bbox['x1']\n",
    "            y1 = crop_img.bbox['y1']\n",
    "            w = crop_img.width\n",
    "            h = crop_img.height\n",
    "            \n",
    "            line = save_format.format(\n",
    "              frame=frame_idx,\n",
    "              id=crop_img.tracker_id,\n",
    "              x1=x1,\n",
    "              y1=y1,\n",
    "              w=w,\n",
    "              h=h\n",
    "            )\n",
    "            # print(f\"Frame {frame_idx}, ID {crop_img.tracker_id}: x1={x1}, y1={y1}, w={w}, h={h}\")\n",
    "            fout.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Detail\n",
    "when modifying LLM.py, this notebook might not refresh as hoped.\n",
    "\n",
    "This cell help refresh the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload LLM module to get latest changes\n",
    "import importlib\n",
    "import LLM\n",
    "importlib.reload(LLM)\n",
    "from LLM import select_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is main executing cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {HOME}\n",
    "result_num = 1\n",
    "EXPRESSION_PATH = os.path.join(HOME, f\"refer-kitti/expression/\") \n",
    "GT_PATH = os.path.join(HOME, f\"refer-kitti/KITTI/labels_with_ids/image_02/\")\n",
    "EXP_PATH = os.path.join(HOME, f\"exps/results_{result_num}/\")\n",
    "LABELS_PATH = os.path.join(HOME, f\"refer-kitti/KITTI/labels_with_ids/image_02/\")\n",
    "\n",
    "EXP_NUM = \"0005\"\n",
    "images_dir = os.path.join(KITTI_PATH, EXP_NUM)\n",
    "sequence_id = os.path.basename(images_dir.rstrip(\"/\"))\n",
    "expression_dir = os.path.join(KITTI_PATH, EXP_NUM)\n",
    "sentence = \"\"\n",
    "if \"refer-kitti-v2\" in EXPRESSION_PATH:\n",
    "    video_ids = [\"0005\", \"0011\", \"0013\", \"0019\"]\n",
    "else:\n",
    "    video_ids = [\"0005\", \"0011\", \"0013\"]\n",
    "\n",
    "for video_id in video_ids[1:2]:\n",
    "    # Uncommnent if full evaluation.\n",
    "    # model = YOLO(\"yolo11n.pt\")\n",
    "    # results = model.track(source=f\"{KITTI_PATH}/{video_id}\", tracker=\"bytetrack.yaml\",persist=True, classes=[0, 2, 3, 5, 7 ])\n",
    "\n",
    "    expression_jsons = sorted(os.listdir(os.path.join(EXPRESSION_PATH, video_id)))\n",
    "\n",
    "    # print(f\"Available expressions: {len(expression_jsons)} files\")\n",
    "    # # Select only the first JSON file instead of all\n",
    "    expression_jsons = [expression_jsons[35]] # adjust the index to select different expressions\n",
    "    print(f\"Selected expression: {expression_jsons[0]}\")\n",
    "    \n",
    "    seq_nums = []\n",
    "    print(expression_jsons)\n",
    "    for expression_json in expression_jsons:\n",
    "        seq_nums.append([video_id, expression_json])\n",
    "    \n",
    "    for seq_num in seq_nums:\n",
    "        write_gt(GT_PATH, EXP_PATH, EXPRESSION_PATH, LABELS_PATH, seq_num[0], seq_num[1].split(\".\")[0])\n",
    "        inference(EXP_PATH, EXPRESSION_PATH, seq_num, results,video_id)\n",
    "        \n",
    "# print(seq_nums[:3])\n",
    "# expression_num = len(seq_nums)\n",
    "# print(expression_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c4deadf"
   },
   "source": [
    "## Create output directory\n",
    "Create a directory to store the annotated frames and the final video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "00c31a61"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = os.path.join(HOME, \"tracking_output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38473844"
   },
   "source": [
    "Save the output video frames geneated by original YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "45a61f61"
   },
   "outputs": [],
   "source": [
    "for i, result in enumerate(results):\n",
    "    # Save the annotated image to the output directory\n",
    "    image_path = os.path.join(output_dir, f\"frame_{i:04d}.jpg\") # Using JPG for smaller size\n",
    "    result.save(filename=image_path)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Saved frame {i} to {image_path}\")\n",
    "print(f\"All annotated frames saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e851dd2b"
   },
   "source": [
    "This is generating video using `predict.txt` generated by inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "165f4183"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Get a list of all saved image files, sorted by name to ensure correct frame order\n",
    "image_files = sorted(glob.glob(os.path.join(output_dir, \"frame_*.jpg\")))\n",
    "\n",
    "if not image_files:\n",
    "    print(\"No frames found to create video.\")\n",
    "else:\n",
    "    # Read the first image to get dimensions\n",
    "    first_frame = cv2.imread(image_files[0])\n",
    "    height, width, layers = first_frame.shape\n",
    "    size = (width, height)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    video_name = os.path.join(output_dir, 'tracking_output.mp4')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can choose other codecs like 'MJPG', 'XVID', 'mp4v'\n",
    "    fps = 10 # Frames per second\n",
    "    out = cv2.VideoWriter(video_name, fourcc, fps, size)\n",
    "\n",
    "    # Write each frame to the video file\n",
    "    for img_file in image_files:\n",
    "        img = cv2.imread(img_file)\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video created successfully at: {video_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict videos Generation\n",
    "By adjusting the `expression` and `EXP_NUM`, it can generate the gt videos in tracking_viz/.\n",
    "\n",
    "Run draw_boxes.py from the notebook cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, shlex\n",
    "\n",
    "EXP_NUM=\"0011\"\n",
    "expression = \"pedestrian\"\n",
    "images_dir = os.path.join(KITTI_PATH, EXP_NUM)\n",
    "predict_file = os.path.join(EXP_PATH, EXP_NUM, expression, \"predict.txt\")\n",
    "\n",
    "# Try to check if the gt file exists and print a few lines\n",
    "try:\n",
    "    if os.path.exists(predict_file):\n",
    "        print(f\"GT file exists: {predict_file}\")\n",
    "        with open(predict_file, 'r') as f:\n",
    "            lines = f.readlines()[:5]\n",
    "            print(f\"First 5 lines of GT file:\")\n",
    "            for line in lines:\n",
    "                print(line.strip())\n",
    "    else:\n",
    "        print(f\"GT file does NOT exist: {predict_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while reading GT file: {e}\")\n",
    "\n",
    "print(f\"\\nImages directory: {images_dir}\")\n",
    "print(f\"Output directory: tracking_viz/predict_viz\")\n",
    "\n",
    "cmd = f'python draw_boxes.py --gt_file \"{predict_file}\" --images_dir \"{images_dir}\" --output_dir tracking_viz/predict_viz'\n",
    "print(f\"\\nRunning: {cmd}\")\n",
    "proc = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n",
    "print(proc.stdout)\n",
    "if proc.returncode != 0:\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    print(proc.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT videos Generation\n",
    "By adjusting the `expression` and `EXP_NUM`, it can generate the gt videos in tracking_viz/.\n",
    "\n",
    "Run draw_boxes.py from the notebook cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, shlex\n",
    "\n",
    "EXP_NUM=\"0005\"\n",
    "expression = \"black-cars-in-the-left\"\n",
    "images_dir = os.path.join(KITTI_PATH, EXP_NUM)\n",
    "gt_file = os.path.join(EXP_PATH, EXP_NUM, expression, \"gt.txt\")\n",
    "\n",
    "# Try to check if the gt file exists and print a few lines\n",
    "try:\n",
    "    if os.path.exists(gt_file):\n",
    "        print(f\"GT file exists: {gt_file}\")\n",
    "        with open(gt_file, 'r') as f:\n",
    "            lines = f.readlines()[:5]\n",
    "            print(f\"First 5 lines of GT file:\")\n",
    "            for line in lines:\n",
    "                print(line.strip())\n",
    "    else:\n",
    "        print(f\"GT file does NOT exist: {gt_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while reading GT file: {e}\")\n",
    "\n",
    "print(f\"\\nImages directory: {images_dir}\")\n",
    "print(f\"Output directory: tracking_viz/gt_viz\")\n",
    "\n",
    "cmd = f'python draw_boxes.py --gt_file \"{gt_file}\" --images_dir \"{images_dir}\" --output_dir tracking_viz/gt_viz'\n",
    "print(f\"\\nRunning: {cmd}\")\n",
    "proc = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n",
    "print(proc.stdout)\n",
    "if proc.returncode != 0:\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    print(proc.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through all files in refer-kitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ByteTrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
